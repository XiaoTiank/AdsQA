<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Project Page of the Benchmark \"AdsQA: Towards Advertisement Video Understanding\"">
  <meta property="og:title" content="AdsQA: Towards Advertisement Video Understanding"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AdsQA: Towards Advertisement Video Understanding</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script><!--   <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AdsQA: Towards Advertisement Video Understanding</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
    }
    .section {
      padding: 4rem 2rem;
    }
    .light {
      background-color: #ffffff;
      color: #111;
    }
    .dark {
      background-color: #f3f4f6;
      color: #111;
    }
    .container {
      max-width: 900px;
      margin: 0 auto;
    }
    h1, h2 {
      margin-bottom: 1rem;
    }
    h1 {
      text-align: center;
      font-size: 2.5rem;
    }
    p, li {
      font-size: 1rem;
      line-height: 1.6;
    }
    .tags {
      text-align: center;
      margin: 1rem 0;
    }
    .tag {
      display: inline-block;
      background: #e5e7eb;
      color: #111;
      padding: 0.4rem 1rem;
      margin: 0.2rem;
      border-radius: 9999px;
      font-size: 0.9rem;
    }
    img {
      display: block;
      margin: 2rem auto;
      max-width: 100%;
      height: auto;
    }
    pre {
      background: #f9fafb;
      padding: 1rem;
      overflow-x: auto;
      border-radius: 5px;
      font-size: 0.85rem;
    }
  </style> -->
</head>
<body>

<!--   <div class="section light">
    <div class="container">
      <h1>AdsQA: Towards Advertisement Video Understanding</h1>
      <p style="text-align:center"><strong>Xinwei Long*, Kai Tian*, Peng Xu, Guoli Jia,  Jingxuan Li, Sa Yang, Yihua Shao, Kaiyan zhang ,Che Jiang, Yang Liu, Jiaheng Ma, Bowen Zhou</strong></p>
      <div class="tags">
        <span class="tag">📄 Paper: Coming Soon</span>
        <span class="tag">📦 Dataset: Coming Soon</span>
        <span class="tag">🤖 Code: Coming Soon</span>
      </div>
      <img src="./assets/video.png" alt="Teaser Image">
    </div>
  </div> -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AdsQA: Towards Advertisement Video Understanding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Xinwei Long</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Kai Tian</a><sup>1*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Peng Xu</a><sup>1</sup>,</span>
              <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Guoli Jia</a><sup>1</sup>,</span> 
            <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jingxuan Li</a><sup>1</sup>,</span>
              <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Sa Yang</a><sup>2</sup>,</span>
            <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yihua Shao</a><sup>3</sup>,</span>
            <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Kaiyan zhang</a><sup>1</sup>,</span>  
            <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Che Jiang</a><sup>1</sup>,</span>  
              <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Hao Xu</a><sup>1</sup>,</span>     
              <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yang Liu</a><sup>1</sup>,</span>  
            <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Jiaheng Ma</a><sup>1</sup>,</span>  
              <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Bowen Zhou</a><sup>1†</sup>,</span>       
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> Tsinghua University</span> ","
                    <span class="author-block"><sup>2</sup> Peiking University</span> ","
                    <span class="author-block"><sup>3</sup> Institute of Automation, Chinese Academy of Sciences</span> 
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small><small><br><sup>†</sup>Indicates Corresponding Author</small>/span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>  

  <div class="section dark">
    <div class="container">
      <h2 style="text-align: center;">Abstract</h2>
      <p style="text-align: justify;">
        Large language models (LLMs) have taken a great step towards AGI. Meanwhile, an increasing number of domainspecific problems such as math and programming boost these general-purpose models to continuously evolve via learning deeper expertise. Now is thus the time further to extend the diversity of specialized applications for knowledgeable LLMs, though collecting high quality data with unexpected and informative tasks is challenging. In this paper, we propose to use advertisement (ad) videos as a challenging test-bed to probe the ability of LLMs in perceiving beyond the objective physical content of common visual domain. Our motivation is to take full advantage of the clue-rich and information-dense ad videos’ traits, e.g., marketing logic, persuasive strategies, and audience engagement. Our contribution is three-fold, (1) To our knowledge, this is the first attempt to use ad videos with well-designed tasks to evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark derived from 1,544 ad videos with 10,962 clips, totaling 21.1 hours, providing 5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that reflects on questions, and generates answers via reward-driven optimization. (3) We benchmark 14 toptier LLMs on AdsQA, and our ReAd-R achieves the stateof-the-art outperforming strong competitors equipped with long-chain reasoning capabilities (e.g., VOT, and MCTSr) by a clear margin.
      </p>
    </div>
  </div>
  <div class="section light">
    <div class="container">
      <h2>Benchmark Overview</h2>
      <img src="./assets/Figure 1.png" alt="Teaser Image">
      <p>
        AdsQA introduces a novel benchmark for understanding advertisement videos using LLMs. These videos are clue-rich,
        persuasion-driven, and semantically dense — ideal for evaluating cognitive-level multimodal reasoning. The benchmark consists
        of 1,544 ad videos, 10,962 clips, and five reasoning tasks: visual concepts, emotion recognition, theme extraction,
        persuasion strategy, and audience modeling. Our proposed model, ReAd-R, a reinforcement-learned ad reasoner, achieves
        state-of-the-art performance across all tasks.
      </p>
    </div>
  </div>
  <div class="section dark">
    <div class="container">
      <h2>Benchmark Tasks</h2>
      <ul>
        <li><strong>Visual Concept Understanding (VU):</strong> Identifying characters, objects, slogans, and relationships.</li>
        <li><strong>Emotion Recognition (ER):</strong> Inferring the emotional tone and audience impact.</li>
        <li><strong>Theme & Message Extraction (TE):</strong> Distilling the ad’s core idea and storyline.</li>
        <li><strong>Persuasion Strategy (PS):</strong> Analyzing rhetorical and visual techniques like humor, metaphor, etc.</li>
        <li><strong>Audience Modeling (AM):</strong> Predicting the intended demographic and psychological profile.</li>
      </ul>
    </div>
  </div>
  <div class="section light">
    <div class="container">
      <h2>Dataset Overview & Statistics</h2>
      <img src="./assets/Figure 2.png" alt="Teaser Image">
      <p style="text-align: justify; font-size: 16px; line-height: 1.8em;">
        The <strong>AdsQA benchmark</strong> introduces a comprehensive, large-scale video QA dataset specifically designed around the complex and information-rich nature of advertisement videos. It offers a diverse and well-structured data source to evaluate LLMs on implicit reasoning tasks.
      </p>
  
      <ul style="font-size: 16px; line-height: 2em;">
        <li>📼 <strong>Total Videos:</strong> 1,544 advertisement videos</li>
        <li>🎞️ <strong>Total Clips:</strong> 10,962 video segments</li>
        <li>🕒 <strong>Duration:</strong> 21.1 hours of curated content</li>
        <li>🌍 <strong>Diverse Domains:</strong> Clothing, food, public service, electronics, healthcare, etc.</li>
        <li>🌐 <strong>Language Coverage:</strong> English (dominant), Spanish, Russian, Japanese, etc.</li>
        <li>📊 <strong>Annotation:</strong> 7,838 QA pairs across five task categories</li>
      </ul>
  
      <p style="text-align: justify; font-size: 16px; line-height: 1.8em; margin-top: 30px;">
        The dataset is constructed using a novel <strong>Role-Played Multi-Agent Annotation</strong> framework that simulates human expert behaviors—including those of marketers, visual designers, and psychologists—to automatically generate rich, specialized, and insightful QA pairs for each advertisement video.
      </p>
  
    </div>
  </div>
  <div class="section dark">
    <div class="container">
      <h2>🧠 Understanding the Need for Specialized Reasoning
      </h2>
      <p style="text-align: justify; font-size: 16px; line-height: 1.8em;">
        Advertisement videos are fundamentally different from typical instructional or user-generated videos. They are designed to convey powerful messages in short time spans—often through symbolism, emotion, metaphor, and subtle psychological cues. These layers of meaning are not explicitly stated; they are intentionally implicit, aiming to influence viewer perception on a subconscious level.
      </p>
  
      <p style="text-align: justify; font-size: 16px; line-height: 1.8em;">
        As a result, reasoning over ad content requires cognitive abilities beyond simple perception. It’s not just about identifying what appears on screen, but understanding <strong>why</strong> it appears, <strong>what</strong> it aims to evoke, and <strong>for whom</strong> it was crafted. Traditional video-language models often fall short when asked to interpret persuasive strategies, emotional triggers, or audience targeting embedded in ad narratives.
      </p>
  
      <p style="text-align: justify; font-size: 16px; line-height: 1.8em;">
        This gap in reasoning capability motivates the need for a new model—one that not only processes visual and textual inputs, but also <strong>reflects, evaluates, and reasons</strong> like a human expert. This is where <strong>ReAd-R</strong>, our Reinforced Ad Reasoner, comes in.
      </p>
    </div>
  </div>

  <div class="section light">
    <div class="container">
      <h2> ReAd-R: Our Reinforced Ad Reasoner Model</h2>
      <img src="./assets/Figure 3.png" alt="Teaser Image" style="max-width: 60%;">
      <p style="text-align: justify; font-size: 16px; line-height: 1.8em;">
        <strong>ReAd-R</strong> is our proposed <strong>reinforcement learning-based reasoning model</strong>, inspired by DeepSeek-R1, tailored for advertisement video understanding. Unlike traditional chain-of-thought methods, ReAd-R reflects on the ad's contents and learns from feedback to generate high-quality answers in a human-like manner.
      </p>
  
      <p style="text-align: justify; font-size: 16px; line-height: 1.8em;">
        It integrates a vision encoder, a lightweight language model, and a multi-stage policy optimization framework. By leveraging rule-based reward modeling, it avoids hallucinations and promotes precise, grounded reasoning. ReAd-R was trained with only 500 annotated ad clips and achieves <strong>state-of-the-art performance</strong> across multiple QA tasks, especially those involving implicit reasoning such as <em>persuasion strategies</em> and <em>emotion recognition</em>.
      </p>
  
      <ul style="line-height: 1.8em; font-size: 16px;">
        <li>🏋️‍♂️ <strong>Trained with GRPO (Generalized Reinforcement Policy Optimization)</strong></li>
        <li>⚙️ <strong>Uses rule-based inclusion and exclusion rewards</strong></li>
        <li>🧠 <strong>Outperforms LLaVA and Qwen2-VL in abstract VideoQA tasks</strong></li>
      </ul>
    </div>
  </div>


  </div>

</body>
</html>
